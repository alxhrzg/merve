# Test Improvement Backlog - Hierarchical Versioning

**Created**: 2025-10-27
**Status**: Post-Phase 5 Review
**Current Test Count**: 46 tests (45 passing, 1 mock issue)
**Current Coverage**: version_control.py 65%

---

## Priority Levels

- **P0 - Critical**: Must fix before production release
- **P1 - High**: Should fix soon, affects important functionality
- **P2 - Medium**: Nice to have, improves coverage/quality
- **P3 - Low**: Future enhancement, not urgent

---

## P0 - Critical (0 items)

**Status**: âœ… All critical items resolved

No critical issues remaining. Core functionality is well-tested and working correctly.

---

## P1 - High Priority (2 items)

### 1. Fix Mock Setup Issue in `test_get_commit_from_git_repo`

**File**: `tests/unit/test_version_control.py:test_get_commit_from_git_repo`

**Issue**: Path traversal mocking fails with `AttributeError: __truediv__`

**Impact**: Medium - Function works correctly (proven by integration tests), but unit test fails

**Why P1**: Test failure reduces confidence, even though it's non-functional

**Effort**: 30 minutes

**Solution Options**:
1. Simplify mock setup to avoid Path traversal issues
2. Use a real temporary directory instead of mocking
3. Skip this specific unit test and rely on integration tests

**Recommended**: Option 2 - Use real temp directory for more realistic testing

**Code Location**: Lines 456-487 in test_version_control.py

---

### 2. Add Multi-Classifier Integration Tests

**File**: New tests needed in `tests/integration/test_hierarchical_versioning.py`

**Gap**: Limited testing of multi-classifier scenarios with different versions

**Impact**: Medium - Multi-classifier support is a key feature

**Why P1**: Important feature that should have better integration coverage

**Effort**: 2-3 hours

**Tests to Add**:
- Multiple classifiers with different version levels (one at v1.0.0, another at v2.5.3)
- Tagging multiple classifiers independently
- Status display for repositories with 3+ classifiers
- Building containers for each classifier independently

**Expected Outcome**: 4-6 new integration tests, coverage of multi-classifier workflows

---

## P2 - Medium Priority (4 items)

### 3. Increase Coverage for Helper Functions

**File**: `mlserver/version_control.py`

**Functions with Low Coverage** (~20-30%):
- `get_latest_tag_info()` - Used extensively but some branches untested
- `check_working_directory_clean()` - Basic tests exist, edge cases missing
- `get_all_classifiers_tag_status()` - Now has some coverage, could be expanded

**Impact**: Low-Medium - These functions work, but edge cases aren't fully covered

**Why P2**: Improves confidence in helper functions, catches potential bugs

**Effort**: 2-3 hours

**Tests to Add**:
- `get_latest_tag_info()` with detached HEAD
- `get_latest_tag_info()` with orphaned tags
- `check_working_directory_clean()` with untracked files vs unstaged vs uncommitted
- `get_all_classifiers_tag_status()` with mixed tag states

**Expected Outcome**: +5-8 unit tests, coverage from 65% to 75%

---

### 4. Add CLI Integration Tests

**File**: New file `tests/integration/test_cli_versioning.py`

**Gap**: CLI commands are not directly tested (only underlying functions)

**Impact**: Medium - CLI is the main user interface

**Why P2**: Ensures CLI commands work end-to-end, not just the underlying functions

**Effort**: 3-4 hours

**Tests to Add**:
- `mlserver tag --classifier <name> patch` (command execution)
- `mlserver tag` (status table display)
- `mlserver version --detailed` (detailed output)
- `mlserver build --classifier <full-tag>` (validation warnings)
- Error messages and exit codes

**Expected Outcome**: 6-8 CLI integration tests, complete user workflow coverage

---

### 5. Add Performance Tests

**File**: New file `tests/performance/test_version_control_performance.py`

**Gap**: No performance testing for large repositories

**Impact**: Low - Performance issues unlikely, but good to validate

**Why P2**: Ensures system scales, prevents regressions

**Effort**: 2 hours

**Tests to Add**:
- Parse repository with 100+ tags
- Get status for repository with 10+ classifiers
- Tag creation performance (baseline measurement)
- Validate operations complete within reasonable time (<1 second)

**Expected Outcome**: 4-5 performance tests, baseline metrics established

---

### 6. Add Edge Case Tests for Git Operations

**File**: `tests/integration/test_git_edge_cases.py`

**Gap**: Unusual git scenarios not tested

**Impact**: Low - Rare scenarios, but good defensive programming

**Why P2**: Handles edge cases gracefully, improves robustness

**Effort**: 2-3 hours

**Tests to Add**:
- Detached HEAD state
- Shallow clones (--depth=1)
- Git worktrees
- Submodules
- Bare repositories
- Merge commits (multiple parents)

**Expected Outcome**: 5-6 edge case tests, better error handling

---

## P3 - Low Priority (3 items)

### 7. Add Parametrized Tests

**File**: Various test files

**Improvement**: Reduce duplication with `@pytest.mark.parametrize`

**Impact**: Low - Code quality improvement, doesn't add new coverage

**Why P3**: Makes tests more maintainable, but doesn't improve coverage

**Effort**: 1-2 hours

**Example**:
```python
@pytest.mark.parametrize("tag,expected", [
    ("sentiment-v1.0.0-mlserver-abc123", "sentiment"),
    ("intent-v2.5.3-mlserver-def456", "intent"),
    ("fraud_detection-v0.1.0-mlserver-789abc", "fraud_detection"),
])
def test_extract_classifier_name_parametrized(tag, expected):
    assert extract_classifier_name(tag) == expected
```

**Expected Outcome**: Reduced test code duplication, easier to add new test cases

---

### 8. Add Documentation Tests

**File**: `tests/docs/test_examples.py`

**Gap**: Code examples in documentation aren't validated

**Impact**: Low - Documentation quality, not functionality

**Why P3**: Prevents documentation drift, ensures examples work

**Effort**: 2 hours

**Tests to Add**:
- Extract code blocks from markdown docs
- Execute them in isolated environments
- Verify they produce expected output
- Test README examples

**Expected Outcome**: 3-5 doc tests, validated documentation

---

### 9. Add Mutation Testing

**Tool**: `mutmut` or `cosmic-ray`

**Purpose**: Verify test quality by mutating code and checking if tests catch changes

**Impact**: Low - Quality metric, doesn't add functional coverage

**Why P3**: Validates test effectiveness, but time-intensive

**Effort**: 4-6 hours (includes setup and analysis)

**Process**:
1. Install mutation testing tool
2. Run mutation tests on version_control.py
3. Analyze survivors (mutations not caught by tests)
4. Add tests for uncaught mutations

**Expected Outcome**: Mutation score >80%, higher confidence in test suite

---

## Summary

**Total Backlog Items**: 9

| Priority | Count | Estimated Effort |
|----------|-------|------------------|
| P0 (Critical) | 0 | 0 hours |
| P1 (High) | 2 | 2.5-3.5 hours |
| P2 (Medium) | 4 | 9-12 hours |
| P3 (Low) | 3 | 7-10 hours |
| **Total** | **9** | **18.5-25.5 hours** |

---

## Recommended Implementation Order

### Sprint 1 (3-4 hours) - High Priority
1. Fix mock setup issue (30 min)
2. Add multi-classifier integration tests (2-3 hours)

### Sprint 2 (4-5 hours) - Increase Coverage
3. Increase coverage for helper functions (2-3 hours)
4. Add CLI integration tests (3-4 hours) *partial*

### Sprint 3 (4-5 hours) - Polish
4. Complete CLI integration tests
5. Add performance tests (2 hours)

### Future Sprints - Nice to Have
6. Add edge case tests for git operations
7. Add parametrized tests
8. Add documentation tests
9. Add mutation testing

---

## Current Test Coverage Breakdown

### Well Covered (>80%)
- `parse_hierarchical_tag()` - ~100%
- `extract_classifier_name()` - ~100%
- `get_tag_commits()` - ~95%
- `tag_version()` (Phase 4 changes) - ~90%

### Good Coverage (60-80%)
- `get_current_version()` - ~75%
- `get_version_for_push()` - ~75%
- `validate_push_readiness()` - ~70% (improved with P1 fixes)
- `get_mlserver_commit_hash()` - ~65%

### Moderate Coverage (40-60%)
- `get_latest_tag_info()` - ~50%
- `get_all_classifiers_tag_status()` - ~45%

### Low Coverage (<40%)
- `safe_push_container()` - ~20% (wrapper function, acceptable)
- CLI command functions - ~10% (tested via integration, acceptable)

---

## Decision Matrix

### When to Implement P1 Items
- Before production release
- Before Phase 7 (GitHub Actions) - multi-classifier support is important
- If you have 2-4 hours of development time

### When to Implement P2 Items
- After Phase 6 (Documentation)
- Before major version release
- If aiming for >75% coverage
- If you have 10+ hours of development time

### When to Implement P3 Items
- After all P1 and P2 items
- During code quality improvement sprints
- If aiming for test suite excellence
- If you have 15+ hours of development time

---

## Notes

1. **Mock Issue**: The single failing unit test (`test_get_commit_from_git_repo`) is a mock setup issue, not a functional problem. Function works correctly in all integration tests.

2. **Multi-Classifier**: This is the highest priority because it's a key feature that could benefit from more comprehensive integration testing.

3. **CLI Testing**: While the underlying functions are well-tested, direct CLI testing would provide additional confidence in the user interface.

4. **Performance**: Not urgent, but establishing baselines now prevents regressions later.

5. **Coverage Goal**: Current 65% is good. Implementing P1 items would bring it to ~70%. Implementing all P2 items would bring it to ~80%.

---

## Status: READY FOR PHASE 6

Current test suite is production-ready with 45 passing tests covering all critical functionality. Backlog items are enhancements for future iterations, not blockers.

**Recommendation**: Proceed to Phase 6 (Documentation) and address backlog items in future sprints.
