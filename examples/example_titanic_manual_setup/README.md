# MLServer FastAPI Wrapper ‚Äî Complete Examples

This directory demonstrates the **complete workflow** for building, versioning, serving, and containerizing ML classifiers using the unified MLServer FastAPI wrapper.

## üöÄ **What's New - Unified Configuration & Automation**

**‚úÖ Single configuration file** (`mlserver.yaml`) replaces dual config approach
**‚úÖ Automatic containerization** with intelligent file detection
**‚úÖ Git-based versioning** and semantic tagging
**‚úÖ Repository-aware naming** for multi-classifier organizations
**‚úÖ Automatic wheel building** for seamless deployment

---

## üìÅ **Directory Structure**

```
examples/
‚îú‚îÄ‚îÄ mlserver.yaml              # ‚úÖ NEW: Unified configuration (everything in one file)
‚îú‚îÄ‚îÄ predictor_catboost.py      # CatBoost predictor implementation
‚îú‚îÄ‚îÄ train_titanic.py           # Model training script
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ artifacts/                 # Model artifacts (generated by training)
‚îÇ   ‚îú‚îÄ‚îÄ catboost_model.pkl
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.pkl
‚îÇ   ‚îî‚îÄ‚îÄ feature_order.json
‚îú‚îÄ‚îÄ DEPRECATED_README.md       # Migration guide for old configs
‚îú‚îÄ‚îÄ config.yaml.deprecated     # Legacy server config (deprecated)
‚îî‚îÄ‚îÄ classifier.yaml.deprecated # Legacy classifier metadata (deprecated)
```

---

## ‚ö° **Quick Start**

### 1. **Setup and Training**
```bash
# Install the package
pip install -e .

# Train the model (creates artifacts/)
python examples/train_titanic.py
```

### 2. **Local Development Server**
```bash
# Serve with unified configuration
ml_server serve examples/mlserver.yaml

# Or use the new default behavior (auto-detects mlserver.yaml)
cd examples && ml_server serve
```

**üåê Test the API:**
```bash
curl -X POST "http://localhost:8000/v1/titanic-survival-predictor/predict" \
  -H "Content-Type: application/json" \
  -d '{"pclass": 3, "sex": "male", "age": 22, "sibsp": 1, "parch": 0, "fare": 7.25, "embarked": "S"}'
```

### 3. **Production Containerization**
```bash
# Build container with automatic wheel building
ml_server build

# Run containerized version
docker run --rm -p 8000:8000 fastapi-mlserver-wrapper-catboostpredictor:latest
```

---

## üîß **Unified Configuration: `mlserver.yaml`**

The new unified configuration combines **all settings** in a single file:

```yaml
# Server Configuration
server:
  title: "Titanic Survival Predictor API"
  host: 0.0.0.0
  port: 8000
  log_level: INFO
  workers: 1  # Process-based scaling (container-friendly)
  cors:
    allow_origins: ["*"]

# Predictor Configuration
predictor:
  module: examples.predictor_catboost
  class_name: CatBoostPredictor
  init_kwargs:
    model_path: "./artifacts/catboost_model.pkl"
    preprocessor_path: "./artifacts/preprocessor.pkl"

# Inference Configuration
inference:
  adapter: records            # records | ndarray | auto
  expose_predict_proba: true
  expose_batch: true
  thread_safe_predict: false

# Observability (Prometheus + Structured Logging)
observability:
  metrics: true               # /metrics endpoint
  structured_logging: true    # JSON logs with correlation IDs
  correlation_ids: true       # Request tracing

# Classifier Metadata (for versioning & containers)
classifier:
  repository: "fastapi-mlserver-wrapper"  # Auto-detected from git
  name: "titanic-survival-predictor"
  version: "1.0.0"
  description: "Titanic survival prediction using CatBoost"

# Model Metadata
model:
  version: "1.0.0"
  trained_at: "2024-01-15T10:30:00Z"
  metrics:
    accuracy: 0.8156
    precision: 0.7941
    recall: 0.6757

# API Versioning
api:
  version: "v1"
  endpoints:
    predict: true
    # Note: batch_predict removed - /predict handles both single and batch
    predict_proba: true

# Build Configuration
build:
  base_image: "python:3.11-slim"
  requirements:
    - "catboost>=1.2"
    - "scikit-learn>=1.1"
  registry: "my-registry.com:5000"
```

---

## üö¢ **Containerization & Deployment**

### **Automatic Container Building**

```bash
# Build with automatic dependency detection
ml_server build

# Explicit wheel source (for development)
ml_server build --mlserver-source /path/to/mlserver/source

# Environment variable approach
export MLSERVER_SOURCE_PATH=/path/to/mlserver
ml_server build
```

**Generated container tags:**
- `fastapi-mlserver-wrapper-catboostpredictor:latest`
- `fastapi-mlserver-wrapper-catboostpredictor:1.0.0`
- `fastapi-mlserver-wrapper-catboostpredictor:1.0.0-abc1234` (with git commit)

### **Intelligent File Detection**

The build process automatically detects and includes:
- **Predictor files**: Python modules with predictor classes
- **Model artifacts**: `.pkl`, `.model`, `.joblib` files
- **Configuration files**: YAML configs and requirements
- **Dependency files**: Only what's needed (reduces image size)

### **Production Deployment**

```bash
# Push to registry
ml_server push --registry my-registry.com:5000

# Deploy with docker-compose, Kubernetes, etc.
docker run -d -p 80:8000 my-registry.com:5000/titanic-classifier:1.0.0
```

---

## üîÑ **API Endpoints (Versioned)**

All endpoints are now **API-versioned** and include the classifier name:

### **Core Endpoints**
- `GET /healthz` - Health check
- `GET /metrics` - Prometheus metrics
- `GET /openapi.json` - OpenAPI specification
- `GET /docs` - Interactive Swagger UI

### **Prediction Endpoints**
- `POST /v1/titanic-survival-predictor/predict` - Single/batch predictions
- `POST /v1/titanic-survival-predictor/predict_proba` - Probability scores

### **Legacy Endpoints (Backward Compatibility)**
- `POST /predict` - Redirects to versioned endpoint
- `POST /predict_proba` - Redirects to versioned endpoint

---

## üìä **Input/Output Schemas**

### **Input Formats**

**Records (Recommended):**
```json
{
  "pclass": 3,
  "sex": "male",
  "age": 22,
  "sibsp": 1,
  "parch": 0,
  "fare": 7.25,
  "embarked": "S"
}
```

**Array Format:**
```json
{
  "payload": {
    "ndarray": [[3, "male", 22, 1, 0, 7.25, "S", "man", true, false]]
  }
}
```

### **Response Format**
```json
{
  "predictions": [0],
  "time_ms": 5.23,
  "model": "CatBoostPredictor",
  "correlation_id": "req_abc123",
  "api_version": "v1"
}
```

---

## üìà **Monitoring & Observability**

### **Prometheus Metrics** (`/metrics`)
- Request counts and latencies
- Model prediction metrics
- Error rates and response codes
- Custom business metrics

### **Structured Logging**
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "correlation_id": "req_abc123",
  "endpoint": "/v1/titanic-survival-predictor/predict",
  "duration_ms": 5.23,
  "model": "CatBoostPredictor"
}
```

### **Health Monitoring**
```bash
# Health check
curl http://localhost:8000/healthz

# Metrics scraping (Prometheus format)
curl http://localhost:8000/metrics
```

---

## üéØ **Performance & Scaling**

### **Process-Based Workers**
```yaml
server:
  workers: 4  # Creates 4 separate processes, each with own model copy
```

**Benefits:**
- True parallelism (not limited by Python GIL)
- Fault isolation between workers
- Container-friendly scaling
- Memory isolation

### **Thread Safety**
```yaml
inference:
  thread_safe_predict: true  # Enable for thread-unsafe models
```

### **Batch Processing**
```bash
# Single prediction
curl -X POST .../predict -d '{"pclass": 3, ...}'

# Batch prediction (using same /predict endpoint)
curl -X POST .../predict -d '[{"pclass": 3, ...}, {"pclass": 1, ...}]'
```

---

## üîß **Development Workflow**

### **1. Model Development**
```bash
# Train and save artifacts
python examples/train_titanic.py

# Test locally
ml_server serve examples/mlserver.yaml
```

### **2. Version Control Integration**
```bash
# Tag your release
git tag v1.0.0
git push origin v1.0.0

# Container tags automatically include git info
ml_server build
# -> Creates: repo-classifier:1.0.0-abc1234
```

### **3. CI/CD Integration**
```bash
# In your CI pipeline
export MLSERVER_SOURCE_PATH=/ci/mlserver-source
ml_server build
ml_server push --registry $REGISTRY_URL
```

---

## üö® **Migration from Legacy Configuration**

**Old approach (deprecated):**
```
config.yaml + classifier.yaml  # Two separate files
```

**New approach:**
```yaml
mlserver.yaml  # Everything unified
```

**Migration steps:**
1. Create `mlserver.yaml` using the example above
2. Copy server settings from old `config.yaml`
3. Copy classifier metadata from old `classifier.yaml`
4. Remove or rename old configuration files

See `DEPRECATED_README.md` for detailed migration guide.

---

## üêõ **Troubleshooting**

### **Common Issues**

**Container build fails with "mlserver wheel not found":**
```bash
# Solution: Provide explicit source path
ml_server build --mlserver-source /path/to/mlserver
# Or set environment variable
export MLSERVER_SOURCE_PATH=/path/to/mlserver
```

**"ModuleNotFoundError: No module named 'examples'":**
```bash
# Solution: Install in development mode
pip install -e .
# Or set PYTHONPATH
export PYTHONPATH="$PWD:$PYTHONPATH"
```

**API endpoints return 404:**
```bash
# Use versioned endpoints
curl http://localhost:8000/v1/titanic-survival-predictor/predict
# Not: http://localhost:8000/predict
```

**Container fails to start:**
```bash
# Check dependencies in requirements.txt
# Ensure artifacts/ directory is included
# Verify predictor module imports work
```

### **Debugging Commands**

```bash
# Check version info
ml_server version

# Validate configuration
python -c "from mlserver.config import AppConfig; import yaml; AppConfig.model_validate(yaml.safe_load(open('mlserver.yaml')))"

# Test container locally
docker run --rm -it fastapi-mlserver-wrapper-catboostpredictor:latest /bin/bash
```

---

## üéì **Advanced Features**

### **Custom Metrics**
```python
# In your predictor
from mlserver.metrics import counter, histogram

class MyPredictor:
    def predict(self, X):
        counter('predictions_total').inc()
        with histogram('prediction_duration').time():
            return self.model.predict(X)
```

### **Model Versioning**
```yaml
model:
  version: "2.1.0"
  trained_at: "2024-01-15T10:30:00Z"
  metrics:
    accuracy: 0.8156
  artifacts:
    model_path: "./artifacts/model_v2.pkl"
    features: "./artifacts/features_v2.json"
```

### **Multi-Environment Configuration**
```bash
# Development
ml_server serve mlserver.dev.yaml

# Staging
ml_server serve mlserver.staging.yaml

# Production
ml_server serve mlserver.prod.yaml
```

---

## üîÆ **Next Steps**

- **Load Testing**: Use the included `load_test_demo.py` for performance analysis
- **Monitoring Stack**: Deploy with Prometheus + Grafana using `docker-compose.monitoring.yml`
- **Security**: Add authentication, rate limiting, and input validation
- **Multi-Model**: Deploy multiple classifiers with different API versions
- **A/B Testing**: Route traffic between model versions

---

**Questions or Issues?** Check the main project README or open an issue for help with advanced deployment scenarios, security configurations, or performance optimization.